{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,col,collect_list\n",
    "from pyspark.sql.types import StringType, ArrayType, DoubleType,IntegerType\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "from hit_rate import HitRate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 20:13:39 WARN Utils: Your hostname, mt-pc resolves to a loopback address: 127.0.1.1; using 192.168.31.100 instead (on interface eno1)\n",
      "22/10/07 20:13:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/mt/Files/conda/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/10/07 20:13:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=MovieRecomender>\n",
      "Spark App Name : MovieRecomender\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .config('spark.ui.showConsoleProgress', 'false')\\\n",
    "                    .appName('MovieRecomender') \\\n",
    "                    .getOrCreate()\n",
    "print(spark.sparkContext)\n",
    "print(\"Spark App Name : \"+ spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 20:14:01 WARN TaskSetManager: Stage 0 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ratings.dat\", engine='python', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "df=df.drop([\"Timestamp\"],axis=1)\n",
    "df.dropna()\n",
    "df = spark.createDataFrame(df)\n",
    "df.createOrReplaceTempView(\"dataset\");\n",
    "df = df.cache()\n",
    "df.count() #force cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "select \n",
    "  A.UserID, A.MovieID, Rating\n",
    "from \n",
    "  (\n",
    "    select \n",
    "      * \n",
    "    from \n",
    "      (\n",
    "        select \n",
    "          distinct(UserID) \n",
    "        from \n",
    "          dataset\n",
    "      ), \n",
    "      (\n",
    "        select \n",
    "          distinct(MovieID) \n",
    "        from \n",
    "          dataset\n",
    "      )\n",
    "  ) as A left outer join dataset as B\n",
    "  on (A.UserID, A.MovieID) = (B.UserID, B.MovieID)\n",
    "'''\n",
    "#full_matrix = spark.sql(sql)\n",
    "#full_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_matrix.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one out for each group in full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loo_matrix = full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(userCol=\"UserID\", itemCol=\"MovieID\", ratingCol=\"Rating\", nonnegative = True, implicitPrefs = False,coldStartStrategy=\"drop\",rank=50,maxIter=15,regParam=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = ParamGridBuilder().addGrid(als.rank,[50]).addGrid(als.maxIter,[15]).addGrid(als.regParam, [0.05] ).build()\n",
    "#thay đổi hyperparams ở đây và chạy lấy kết quả viết báo cáo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Rating\", predictionCol=\"prediction\") \n",
    "ndcg = RankingEvaluator(labelCol=\"RealRank\", predictionCol=\"recommendations\",metricName=\"ndcgAtK\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=als, estimatorParamMaps=grid_search, evaluator=rmse, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 20:14:02 WARN TaskSetManager: Stage 2 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/10/07 20:14:03 WARN TaskSetManager: Stage 3 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/10/07 20:14:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/10/07 20:14:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setCheckpointDir('checkpoint/')\n",
    "#model=cv.fit(train)\n",
    "model=als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 20:48:22 WARN TaskSetManager: Stage 290 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/10/07 20:48:22 WARN TaskSetManager: Stage 291 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|UserID|     recommendations|            RealRank|\n",
      "+------+--------------------+--------------------+\n",
      "|    26|[572.0, 3106.0, 1...|[1682.0, 1259.0, ...|\n",
      "|    29|[572.0, 1997.0, 3...|[2993.0, 1262.0, ...|\n",
      "|   474|[2905.0, 1068.0, ...|[1250.0, 589.0, 1...|\n",
      "|   964|[3092.0, 2997.0, ...|[2997.0, 588.0, 1...|\n",
      "|  1677|[37.0, 2571.0, 23...|[1617.0, 2628.0, ...|\n",
      "|  1697|[572.0, 3114.0, 3...|[2987.0, 3798.0, ...|\n",
      "|  1806|[572.0, 598.0, 30...|[3793.0, 2058.0, ...|\n",
      "|  1950|[1199.0, 608.0, 3...|[720.0, 1.0, 595....|\n",
      "|  2040|[3870.0, 2129.0, ...|[3798.0, 3948.0, ...|\n",
      "|  2214|[3092.0, 608.0, 2...|[1.0, 2064.0, 126...|\n",
      "|  2250|[2997.0, 2959.0, ...|[1252.0, 2997.0, ...|\n",
      "|  2453|[572.0, 527.0, 67...|[1249.0, 1250.0, ...|\n",
      "|  2509|[1545.0, 326.0, 3...|[858.0, 3178.0, 5...|\n",
      "|  2529|[572.0, 2562.0, 3...|[2987.0, 1250.0, ...|\n",
      "|  2927|[110.0, 572.0, 37...|[1275.0, 1291.0, ...|\n",
      "|  3091|[572.0, 213.0, 11...|[593.0, 2263.0, 1...|\n",
      "|  3506|[1206.0, 1423.0, ...|[2997.0, 3000.0, ...|\n",
      "|  3764|[2594.0, 572.0, 1...|[3006.0, 923.0, 2...|\n",
      "|  4590|[572.0, 527.0, 14...|[296.0, 50.0, 226...|\n",
      "|  4823|[50.0, 668.0, 778...|[3794.0, 2064.0, ...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getRank(a):\n",
    "    ret=[]\n",
    "    for i in a:\n",
    "        ret.append(float(i.MovieID))\n",
    "    return ret\n",
    "convertUDF = udf(lambda z: getRank(z),ArrayType(DoubleType()))\n",
    "\n",
    "def toDouble(a):\n",
    "    return [float(i) for i in a]\n",
    "toDoubleUDF = udf(lambda z: toDouble(z),ArrayType(DoubleType()))\n",
    "\n",
    "tempt=df.sort(col('Rating').desc()).groupBy(\"UserID\").agg(collect_list('MovieID').alias(\"RealRank\"))\n",
    "tempt=tempt.withColumn(\"RealRank\",toDoubleUDF(col(\"RealRank\")))\n",
    "\n",
    "rec=model.recommendForAllUsers(3952).join(tempt,\"UserID\",\"inner\")\n",
    "rec=rec.withColumn(\"recommendations\",convertUDF(col(\"recommendations\")))\n",
    "rec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cv_fitted.bestModel.rank, cv_fitted.bestModel._java_obj.parent().getMaxIter(),cv_fitted.bestModel._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 20:48:38 WARN TaskSetManager: Stage 322 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/10/07 20:48:41 WARN TaskSetManager: Stage 327 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/10/07 20:48:41 WARN TaskSetManager: Stage 328 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565193235068412 0.2677876035593543\n"
     ]
    }
   ],
   "source": [
    "predictions=model.transform(test).na.drop()\n",
    "print(rmse.evaluate(predictions),ndcg.evaluate(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 18:38:29 WARN TaskSetManager: Stage 667 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/10/07 18:38:29 WARN TaskSetManager: Stage 668 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_evaluate() missing 1 required positional argument: 'gt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mt/Desktop/MovieRecomender/notebook.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mt/Desktop/MovieRecomender/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m als\u001b[39m.\u001b[39mfit(train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mt/Desktop/MovieRecomender/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(test)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mt/Desktop/MovieRecomender/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m hr_evaluator\u001b[39m.\u001b[39;49mevaluate(predictions)\n",
      "File \u001b[0;32m~/Files/conda/lib/python3.9/site-packages/pyspark/ml/evaluation.py:84\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m     83\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(dataset)\n\u001b[1;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "\u001b[0;31mTypeError\u001b[0m: _evaluate() missing 1 required positional argument: 'gt'"
     ]
    }
   ],
   "source": [
    "#hr_evaluator = HitRate(predictionCol='prediction', labelCol='rating', userCol='userId')\n",
    "#model = als.fit(train)\n",
    "#predictions = model.transform(test)\n",
    "#hr_evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/07 20:48:09 WARN TaskSetManager: Stage 286 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/10/07 20:48:10 WARN TaskSetManager: Stage 287 contains a task of very large size (1581 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|UserID|            RealRank|\n",
      "+------+--------------------+\n",
      "|    26|[1682.0, 1259.0, ...|\n",
      "|    29|[2993.0, 1262.0, ...|\n",
      "|   474|[1250.0, 589.0, 1...|\n",
      "|   964|[2997.0, 588.0, 1...|\n",
      "|  1677|[1617.0, 2628.0, ...|\n",
      "|  1697|[2987.0, 3798.0, ...|\n",
      "|  1806|[3793.0, 2058.0, ...|\n",
      "|  1950|[720.0, 1.0, 595....|\n",
      "|  2040|[3798.0, 3948.0, ...|\n",
      "|  2214|[1.0, 2064.0, 126...|\n",
      "|  2250|[1252.0, 2997.0, ...|\n",
      "|  2453|[1249.0, 1250.0, ...|\n",
      "|  2509|[858.0, 3178.0, 5...|\n",
      "|  2529|[2987.0, 1250.0, ...|\n",
      "|  2927|[1275.0, 1291.0, ...|\n",
      "|  3091|[593.0, 2263.0, 1...|\n",
      "|  3506|[2997.0, 3000.0, ...|\n",
      "|  3764|[3006.0, 923.0, 2...|\n",
      "|  4590|[296.0, 50.0, 226...|\n",
      "|  4823|[3794.0, 2064.0, ...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempt=df.sort(col('Rating').desc()).groupBy(\"UserID\").agg(collect_list('MovieID').alias(\"RealRank\"))\n",
    "tempt=tempt.withColumn(\"RealRank\",toDoubleUDF(col(\"RealRank\")))\n",
    "tempt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a2d9fd7e9f89f7b2e17511071ff85730f142684744a4d09a2bb99de17d2a3a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
