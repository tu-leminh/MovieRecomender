{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from hit_rate import HitRate\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/06 02:24:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/06 02:24:43 WARN util.Utils: Service 'sparkDriver' could not bind on port 7080. Attempting port 7081.\n",
      "22/10/06 02:24:45 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=MovieRecomender>\n",
      "Spark App Name : MovieRecomender\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .config('spark.ui.showConsoleProgress', 'false')\\\n",
    "                    .appName('MovieRecomender') \\\n",
    "                    .getOrCreate()\n",
    "print(spark.sparkContext)\n",
    "print(\"Spark App Name : \"+ spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000209"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option(\"sep\", \"::\").csv(\"file:///data/zlb/zlb-account/hanh4/hanh4/data/ratings.dat\")\n",
    "df.na.drop()\n",
    "df = df.toDF(*[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "df.createOrReplaceTempView(\"dataset\");\n",
    "df = df.cache()\n",
    "df.count() #force cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserID|MovieID|Rating|RowNumber|\n",
      "+------+-------+------+---------+\n",
      "|  1090|   1088|  null|        1|\n",
      "|  1090|   1139|  null|        2|\n",
      "|  1090|    128|  null|        3|\n",
      "|  1090|   1297|  null|        4|\n",
      "|  1090|   1398|  null|        5|\n",
      "|  1090|   1651|  null|        6|\n",
      "|  1090|   1837|  null|        7|\n",
      "|  1090|   1926|  null|        8|\n",
      "|  1090|    199|  null|        9|\n",
      "|  1090|   2291|  null|       10|\n",
      "|  1090|   2711|  null|       11|\n",
      "|  1090|   2777|  null|       12|\n",
      "|  1090|   2817|  null|       13|\n",
      "|  1090|   2876|  null|       14|\n",
      "|  1090|   2953|  null|       15|\n",
      "|  1090|   3074|  null|       16|\n",
      "|  1090|    562|  null|       17|\n",
      "|  1090|    657|  null|       18|\n",
      "|  1090|    815|  null|       19|\n",
      "|  1090|    898|  null|       20|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "select \n",
    "  A.UserID, A.MovieID, Rating, ROW_NUMBER() OVER (partition by A.UserID order by Rating desc) as RowNumber\n",
    "from \n",
    "  (\n",
    "    select \n",
    "      * \n",
    "    from \n",
    "      (\n",
    "        select \n",
    "          distinct(UserID) \n",
    "        from \n",
    "          dataset\n",
    "      ), \n",
    "      (\n",
    "        select \n",
    "          distinct(MovieID) \n",
    "        from \n",
    "          dataset\n",
    "      )\n",
    "  ) as A left outer join dataset as B\n",
    "  on (A.UserID, A.MovieID) = (B.UserID, B.MovieID)\n",
    "'''\n",
    "full_matrix = spark.sql(sql)\n",
    "full_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22384240"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_matrix.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one out for each group in full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_matrix = full_matrix\n",
    "loo_train_matrix = full_matrix.filter(F.col(\"RowNumber\") == 1)\n",
    "loo_test_matrix = full_matrix.filter(F.col(\"RowNumber\") != 1)\n",
    "\n",
    "loo_train_matrix.show()\n",
    "loo_test_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(userCol=\"UserID\", itemCol=\"MovieID\", ratingCol=\"Rating\", nonnegative = True, implicitPrefs = False,coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = ParamGridBuilder().addGrid(als.rank,[30,50]).addGrid(als.maxIter,[15,20,50]).addGrid(als.regParam, [0.05] ).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Rating\", predictionCol=\"prediction\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=als, estimatorParamMaps=grid_search, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir('checkpoint/')\n",
    "cv_fitted=cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_fitted.bestModel.rank, cv_fitted.bestModel._java_obj.parent().getMaxIter(),cv_fitted.bestModel._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(cv_fitted.transform(test).na.drop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_evaluator = HitRate(predictionCol='prediction', labelCol='rating', userCol='userId')\n",
    "model = als.fit(train)\n",
    "predictions = model.transform(test)\n",
    "hr_evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
